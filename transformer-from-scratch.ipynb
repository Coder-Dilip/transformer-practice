{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84740,"sourceType":"datasetVersion","datasetId":46601}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:54:53.016159Z","iopub.execute_input":"2025-05-25T16:54:53.016730Z","iopub.status.idle":"2025-05-25T16:54:53.288366Z","shell.execute_reply.started":"2025-05-25T16:54:53.016708Z","shell.execute_reply":"2025-05-25T16:54:53.287641Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/wikipedia-sentences/wikisent2.txt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"# Load the raw lines from the .txt file\ndef load_wikipedia_sentences(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        sentences = f.read().splitlines()\n    return sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T16:54:55.478017Z","iopub.execute_input":"2025-05-25T16:54:55.478553Z","iopub.status.idle":"2025-05-25T16:54:55.482958Z","shell.execute_reply.started":"2025-05-25T16:54:55.478513Z","shell.execute_reply":"2025-05-25T16:54:55.482047Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load the dataset\nfile_path = '/kaggle/input/wikipedia-sentences/wikisent2.txt'\nsentences = load_wikipedia_sentences(file_path)\n\n# Preview a few examples\nprint(\"Total sentences:\", len(sentences))\nprint(\"Example sentences:\")\nfor i in range(10):\n    print(f\"{i+1}: {sentences[i]}\")\ntype(sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:08.448140Z","iopub.execute_input":"2025-05-25T17:04:08.448845Z","iopub.status.idle":"2025-05-25T17:04:12.232359Z","shell.execute_reply.started":"2025-05-25T17:04:08.448820Z","shell.execute_reply":"2025-05-25T17:04:12.231773Z"}},"outputs":[{"name":"stdout","text":"Total sentences: 7871825\nExample sentences:\n1: 0.000123, which corresponds to a distance of 705 Mly, or 216 Mpc.\n2: 000webhost is a free web hosting service, operated by Hostinger.\n3: 0010x0010 is a Dutch-born audiovisual artist, currently living in Los Angeles.\n4: 0-0-1-3 is an alcohol abuse prevention program developed in 2004 at Francis E. Warren Air Force Base based on research by the National Institute on Alcohol Abuse and Alcoholism regarding binge drinking in college students.\n5: 0.01 is the debut studio album of H3llb3nt, released on February 20, 1996 by Fifth Colvmn Records.\n6: 001 of 3 February 1997, which was signed between the Government of the Republic of Rwanda, and FAPADER.\n7: 003230 is a South Korean food manufacturer.\n8: 0.04%Gas molecules in soil are in continuous thermal motion according to the kinetic theory of gasses, there is also collision between molecules - a random walk.\n9: 0.04% of the votes were invalid.\n10: 005.1999.06 is the fifth studio album by the South Korean singer and actress Uhm Jung-hwa.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def preprocess_sentences(sentences):\n    processed = [s.strip().lower() for s in sentences if len(s.strip()) > 0]\n    return processed\n\n# Preprocess sentences\nsentences = preprocess_sentences(sentences[:100000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:23.465927Z","iopub.execute_input":"2025-05-25T17:04:23.466423Z","iopub.status.idle":"2025-05-25T17:04:23.838036Z","shell.execute_reply.started":"2025-05-25T17:04:23.466379Z","shell.execute_reply":"2025-05-25T17:04:23.837466Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# hugging face tokenizer\n!pip install -q tokenizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:27.329599Z","iopub.execute_input":"2025-05-25T17:04:27.330167Z","iopub.status.idle":"2025-05-25T17:04:30.382003Z","shell.execute_reply.started":"2025-05-25T17:04:27.330147Z","shell.execute_reply":"2025-05-25T17:04:30.381040Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\nfrom tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n\n# Save sentences to a file, required for training\nwith open(\"train_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n    for s in sentences:\n        f.write(s.strip() + \"\\n\")\n\n# Initialize a tokenizer with a WordPiece model\ntokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n\n# Normalization: lowercase, remove accents\ntokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n\n# Pre-tokenizer: basic whitespace splitting\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n\n# Trainer: WordPiece trainer\ntrainer = trainers.WordPieceTrainer(\n    vocab_size=30_000,\n    min_frequency=2,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# Train tokenizer\ntokenizer.train([\"train_sentences.txt\"], trainer)\n\n# Optional: save tokenizer for later use\ntokenizer.save(\"custom_wordpiece_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:31.094679Z","iopub.execute_input":"2025-05-25T17:04:31.094973Z","iopub.status.idle":"2025-05-25T17:04:35.203165Z","shell.execute_reply.started":"2025-05-25T17:04:31.094947Z","shell.execute_reply":"2025-05-25T17:04:35.202476Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Reload tokenizer\ntokenizer = Tokenizer.from_file(\"custom_wordpiece_tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:39.327107Z","iopub.execute_input":"2025-05-25T17:04:39.327375Z","iopub.status.idle":"2025-05-25T17:04:39.365045Z","shell.execute_reply.started":"2025-05-25T17:04:39.327357Z","shell.execute_reply":"2025-05-25T17:04:39.364439Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Encode a sample\nsample = \"the curious fox jumped over the lazy dog\"\noutput = tokenizer.encode(sample)\n\nprint(\"Tokens:\", output.tokens)\nprint(\"Token IDs:\", output.ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:42.371576Z","iopub.execute_input":"2025-05-25T17:04:42.372083Z","iopub.status.idle":"2025-05-25T17:04:42.376655Z","shell.execute_reply.started":"2025-05-25T17:04:42.372061Z","shell.execute_reply":"2025-05-25T17:04:42.375835Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['the', 'cur', '##ious', 'fox', 'jump', '##ed', 'over', 'the', 'laz', '##y', 'dog']\nToken IDs: [139, 2564, 999, 4839, 10135, 148, 669, 139, 12613, 94, 4524]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Masking","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nMASK_PROB = 0.15        # 15% masking\nMAX_LEN = 64            # Fixed sequence length\nMASK_TOKEN_ID = tokenizer.token_to_id(\"[MASK]\")\nPAD_TOKEN_ID = tokenizer.token_to_id(\"[PAD]\")\nCLS_TOKEN_ID = tokenizer.token_to_id(\"[CLS]\")\nSEP_TOKEN_ID = tokenizer.token_to_id(\"[SEP]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:45.308628Z","iopub.execute_input":"2025-05-25T17:04:45.309342Z","iopub.status.idle":"2025-05-25T17:04:45.313496Z","shell.execute_reply.started":"2025-05-25T17:04:45.309319Z","shell.execute_reply":"2025-05-25T17:04:45.312757Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import random\n\ndef mask_input(input_ids):\n    labels = [-100] * len(input_ids)  # default ignore index\n\n    for i in range(1, len(input_ids) - 1):  # avoid masking [CLS] or [SEP]\n        if random.random() < MASK_PROB:\n            labels[i] = input_ids[i]\n\n            # 80% replace with [MASK]\n            if random.random() < 0.8:\n                input_ids[i] = MASK_TOKEN_ID\n            # 10% replace with random token\n            elif random.random() < 0.5:\n                input_ids[i] = random.randint(0, tokenizer.get_vocab_size() - 1)\n            # 10% leave unchanged\n    return input_ids, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:50.346155Z","iopub.execute_input":"2025-05-25T17:04:50.346795Z","iopub.status.idle":"2025-05-25T17:04:50.351557Z","shell.execute_reply.started":"2025-05-25T17:04:50.346771Z","shell.execute_reply":"2025-05-25T17:04:50.350850Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class MLMDataset(Dataset):\n    def __init__(self, sentences, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.sentences = sentences\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        text = self.sentences[idx]\n        tokens = self.tokenizer.encode(text)\n\n        input_ids = [CLS_TOKEN_ID] + tokens.ids[:self.max_len-2] + [SEP_TOKEN_ID]\n        input_ids += [PAD_TOKEN_ID] * (self.max_len - len(input_ids))\n\n        attention_mask = [1 if id != PAD_TOKEN_ID else 0 for id in input_ids]\n\n        masked_input_ids, labels = mask_input(input_ids.copy())\n\n        return {\n            'input_ids': torch.tensor(masked_input_ids),\n            'attention_mask': torch.tensor(attention_mask),\n            'labels': torch.tensor(labels)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:52.054384Z","iopub.execute_input":"2025-05-25T17:04:52.055128Z","iopub.status.idle":"2025-05-25T17:04:52.064033Z","shell.execute_reply.started":"2025-05-25T17:04:52.055104Z","shell.execute_reply":"2025-05-25T17:04:52.063434Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# You can split sentences into train/val later\ndataset = MLMDataset(sentences, tokenizer, MAX_LEN)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n\n# Preview one batch\nbatch = next(iter(dataloader))\nprint(\"Input IDs:\", batch['input_ids'].shape)\nprint(\"Labels:\", batch['labels'].shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:54.445601Z","iopub.execute_input":"2025-05-25T17:04:54.446168Z","iopub.status.idle":"2025-05-25T17:04:54.658458Z","shell.execute_reply.started":"2025-05-25T17:04:54.446146Z","shell.execute_reply":"2025-05-25T17:04:54.657677Z"}},"outputs":[{"name":"stdout","text":"Input IDs: torch.Size([64, 64])\nLabels: torch.Size([64, 64])\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"# model making","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass TransformerEmbeddings(nn.Module):\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n        self.position_embed = nn.Embedding(max_len, embed_dim)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, input_ids):\n        seq_len = input_ids.size(1)\n        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n        x = self.token_embed(input_ids) + self.position_embed(positions)\n        return self.layer_norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:56.794750Z","iopub.execute_input":"2025-05-25T17:04:56.795482Z","iopub.status.idle":"2025-05-25T17:04:56.800733Z","shell.execute_reply.started":"2025-05-25T17:04:56.795453Z","shell.execute_reply":"2025-05-25T17:04:56.799848Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, heads, ff_hidden_dim, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n        self.attn_norm = nn.LayerNorm(embed_dim)\n\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.GELU(),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n        self.ff_norm = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, attention_mask):\n        # x: (B, S, E)\n        # attention_mask: (B, S) with 1 for real token, 0 for pad\n        key_padding_mask = (attention_mask == 0)  # shape (B, S), True = pad\n        attn_output, _ = self.attn(\n            x, x, x,\n            key_padding_mask=key_padding_mask  # shape: (B, S)\n        )\n    \n        x = self.attn_norm(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        x = self.ff_norm(x + self.dropout(ff_output))\n        return x\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:04:58.946124Z","iopub.execute_input":"2025-05-25T17:04:58.946673Z","iopub.status.idle":"2025-05-25T17:04:58.952519Z","shell.execute_reply.started":"2025-05-25T17:04:58.946651Z","shell.execute_reply":"2025-05-25T17:04:58.951686Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class MLMTransformer(nn.Module):\n    def __init__(self, vocab_size, max_len, embed_dim=128, heads=4, depth=4, ff_dim=512):\n        super().__init__()\n        self.embedding = TransformerEmbeddings(vocab_size, embed_dim, max_len)\n        self.encoder = nn.ModuleList([\n            TransformerBlock(embed_dim, heads, ff_dim) for _ in range(depth)\n        ])\n        self.mlm_head = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embedding(input_ids)\n\n        # Convert attention mask to bool where 0 = True (masked), 1 = False\n        attn_mask = (attention_mask == 0)\n\n        for block in self.encoder:\n            x = block(x, attn_mask)\n\n        logits = self.mlm_head(x)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:05:01.087281Z","iopub.execute_input":"2025-05-25T17:05:01.088036Z","iopub.status.idle":"2025-05-25T17:05:01.093294Z","shell.execute_reply.started":"2025-05-25T17:05:01.088011Z","shell.execute_reply":"2025-05-25T17:05:01.092576Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\n\n# Check device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:05:03.235241Z","iopub.execute_input":"2025-05-25T17:05:03.235542Z","iopub.status.idle":"2025-05-25T17:05:03.240243Z","shell.execute_reply.started":"2025-05-25T17:05:03.235519Z","shell.execute_reply":"2025-05-25T17:05:03.239445Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Get vocab size from tokenizer\nvocab_size = tokenizer.get_vocab_size()\n\n# Instantiate model\nmodel = MLMTransformer(vocab_size=vocab_size, max_len=MAX_LEN).to(device)\n\n# Loss: ignore index -100 where labels are not masked\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:05:04.673727Z","iopub.execute_input":"2025-05-25T17:05:04.674385Z","iopub.status.idle":"2025-05-25T17:05:04.760444Z","shell.execute_reply.started":"2025-05-25T17:05:04.674363Z","shell.execute_reply":"2025-05-25T17:05:04.759870Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"EPOCHS = 3\n\nmodel.train()\n\nfor epoch in range(EPOCHS):\n    loop = tqdm(dataloader, leave=True)\n    total_loss = 0\n\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        optimizer.zero_grad()\n\n        logits = model(input_ids, attention_mask)\n\n        # Reshape for loss: [batch*seq_len, vocab]\n        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        loop.set_description(f\"Epoch {epoch+1}\")\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T17:05:07.121382Z","iopub.execute_input":"2025-05-25T17:05:07.121894Z"}},"outputs":[{"name":"stderr","text":"Epoch 1:  43%|████▎     | 677/1563 [02:46<03:31,  4.20it/s, loss=nan]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"mlm_transformer.pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.save(\"mlm_tokenizer.json\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
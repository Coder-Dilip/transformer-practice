{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84740,"sourceType":"datasetVersion","datasetId":46601}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:02.831244Z","iopub.execute_input":"2025-05-25T19:01:02.831832Z","iopub.status.idle":"2025-05-25T19:01:02.841223Z","shell.execute_reply.started":"2025-05-25T19:01:02.831809Z","shell.execute_reply":"2025-05-25T19:01:02.840528Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/wikipedia-sentences/wikisent2.txt\n","output_type":"stream"}],"execution_count":139},{"cell_type":"markdown","source":"# Loading dataset","metadata":{}},{"cell_type":"code","source":"# Load the raw lines from the .txt file\ndef load_wikipedia_sentences(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        sentences = f.read().splitlines()\n    return sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:05.668827Z","iopub.execute_input":"2025-05-25T19:01:05.669386Z","iopub.status.idle":"2025-05-25T19:01:05.674037Z","shell.execute_reply.started":"2025-05-25T19:01:05.669357Z","shell.execute_reply":"2025-05-25T19:01:05.673004Z"}},"outputs":[],"execution_count":140},{"cell_type":"code","source":"# Load the dataset\nfile_path = '/kaggle/input/wikipedia-sentences/wikisent2.txt'\nsentences = load_wikipedia_sentences(file_path)\n\n# Preview a few examples\nprint(\"Total sentences:\", len(sentences))\nprint(\"Example sentences:\")\nfor i in range(10):\n    print(f\"{i+1}: {sentences[i]}\")\ntype(sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:08.043561Z","iopub.execute_input":"2025-05-25T19:01:08.043803Z","iopub.status.idle":"2025-05-25T19:01:11.811716Z","shell.execute_reply.started":"2025-05-25T19:01:08.043788Z","shell.execute_reply":"2025-05-25T19:01:11.811158Z"}},"outputs":[{"name":"stdout","text":"Total sentences: 7871825\nExample sentences:\n1: 0.000123, which corresponds to a distance of 705 Mly, or 216 Mpc.\n2: 000webhost is a free web hosting service, operated by Hostinger.\n3: 0010x0010 is a Dutch-born audiovisual artist, currently living in Los Angeles.\n4: 0-0-1-3 is an alcohol abuse prevention program developed in 2004 at Francis E. Warren Air Force Base based on research by the National Institute on Alcohol Abuse and Alcoholism regarding binge drinking in college students.\n5: 0.01 is the debut studio album of H3llb3nt, released on February 20, 1996 by Fifth Colvmn Records.\n6: 001 of 3 February 1997, which was signed between the Government of the Republic of Rwanda, and FAPADER.\n7: 003230 is a South Korean food manufacturer.\n8: 0.04%Gas molecules in soil are in continuous thermal motion according to the kinetic theory of gasses, there is also collision between molecules - a random walk.\n9: 0.04% of the votes were invalid.\n10: 005.1999.06 is the fifth studio album by the South Korean singer and actress Uhm Jung-hwa.\n","output_type":"stream"},{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"def preprocess_sentences(sentences):\n    processed = [s.strip().lower() for s in sentences if len(s.strip()) > 0]\n    return processed\n\n# Preprocess sentences\nsentences = preprocess_sentences(sentences[:300000])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:15.902520Z","iopub.execute_input":"2025-05-25T19:01:15.902785Z","iopub.status.idle":"2025-05-25T19:01:16.353248Z","shell.execute_reply.started":"2025-05-25T19:01:15.902766Z","shell.execute_reply":"2025-05-25T19:01:16.352667Z"}},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"# hugging face tokenizer\n!pip install -q tokenizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:01:20.301706Z","iopub.execute_input":"2025-05-25T19:01:20.301977Z","iopub.status.idle":"2025-05-25T19:01:23.334041Z","shell.execute_reply.started":"2025-05-25T19:01:20.301956Z","shell.execute_reply":"2025-05-25T19:01:23.333234Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":143},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\nfrom tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence\n\n# Save sentences to a file, required for training\nwith open(\"train_sentences.txt\", \"w\", encoding=\"utf-8\") as f:\n    for s in sentences:\n        f.write(s.strip() + \"\\n\")\n\n# Initialize a tokenizer with a WordPiece model\ntokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n\n# Normalization: lowercase, remove accents\ntokenizer.normalizer = Sequence([NFD(), Lowercase(), StripAccents()])\n\n# Pre-tokenizer: basic whitespace splitting\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n\n# Trainer: WordPiece trainer\ntrainer = trainers.WordPieceTrainer(\n    vocab_size=10_000,\n    min_frequency=2,\n    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n)\n\n# Train tokenizer\ntokenizer.train([\"train_sentences.txt\"], trainer)\n\n# Optional: save tokenizer for later use\ntokenizer.save(\"custom_wordpiece_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:02:13.689327Z","iopub.execute_input":"2025-05-25T19:02:13.689739Z","iopub.status.idle":"2025-05-25T19:02:21.319011Z","shell.execute_reply.started":"2025-05-25T19:02:13.689713Z","shell.execute_reply":"2025-05-25T19:02:21.318344Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":144},{"cell_type":"code","source":"# Reload tokenizer\ntokenizer = Tokenizer.from_file(\"custom_wordpiece_tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:02:24.837516Z","iopub.execute_input":"2025-05-25T19:02:24.838026Z","iopub.status.idle":"2025-05-25T19:02:24.852109Z","shell.execute_reply.started":"2025-05-25T19:02:24.838007Z","shell.execute_reply":"2025-05-25T19:02:24.851251Z"}},"outputs":[],"execution_count":145},{"cell_type":"code","source":"# Encode a sample\nsample = \"the curious fox jumped over the lazy dog\"\noutput = tokenizer.encode(sample)\n\nprint(\"Tokens:\", output.tokens)\nprint(\"Token IDs:\", output.ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:02:28.306390Z","iopub.execute_input":"2025-05-25T19:02:28.306658Z","iopub.status.idle":"2025-05-25T19:02:28.311360Z","shell.execute_reply.started":"2025-05-25T19:02:28.306641Z","shell.execute_reply":"2025-05-25T19:02:28.310649Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['the', 'cur', '##ious', 'fox', 'jump', '##ed', 'over', 'the', 'la', '##zy', 'dog']\nToken IDs: [142, 1573, 643, 4549, 5389, 146, 611, 142, 1138, 3379, 3351]\n","output_type":"stream"}],"execution_count":146},{"cell_type":"markdown","source":"# Masking","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nMASK_PROB = 0.15        # 15% masking\nMAX_LEN = 32           # Fixed sequence length\nMASK_TOKEN_ID = tokenizer.token_to_id(\"[MASK]\")\nPAD_TOKEN_ID = tokenizer.token_to_id(\"[PAD]\")\nCLS_TOKEN_ID = tokenizer.token_to_id(\"[CLS]\")\nSEP_TOKEN_ID = tokenizer.token_to_id(\"[SEP]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:02:45.337150Z","iopub.execute_input":"2025-05-25T19:02:45.337909Z","iopub.status.idle":"2025-05-25T19:02:45.342013Z","shell.execute_reply.started":"2025-05-25T19:02:45.337884Z","shell.execute_reply":"2025-05-25T19:02:45.341299Z"}},"outputs":[],"execution_count":149},{"cell_type":"code","source":"import random\ndef mask_input(input_ids, tokenizer):\n    MASK_TOKEN_ID = tokenizer.token_to_id(\"[MASK]\")\n    vocab_size = tokenizer.get_vocab_size()\n    \n    labels = [-100] * len(input_ids)\n    \n    for i in range(1, len(input_ids) - 1):  # avoid masking [CLS] or [SEP]\n        if input_ids[i] == tokenizer.token_to_id(\"[PAD]\"):  # don't mask padding\n            continue\n            \n        if random.random() < 0.15:  # 15% masking probability\n            labels[i] = input_ids[i]\n            \n            rand_val = random.random()\n            if rand_val < 0.8:  # 80% replace with [MASK]\n                input_ids[i] = MASK_TOKEN_ID\n            elif rand_val < 0.9:  # 10% replace with random token\n                # Ensure we don't use special tokens for random replacement\n                input_ids[i] = random.randint(5, vocab_size - 1)  # Skip special tokens\n            # 10% leave unchanged\n    \n    return input_ids, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:02:47.319216Z","iopub.execute_input":"2025-05-25T19:02:47.319574Z","iopub.status.idle":"2025-05-25T19:02:47.324909Z","shell.execute_reply.started":"2025-05-25T19:02:47.319552Z","shell.execute_reply":"2025-05-25T19:02:47.324185Z"}},"outputs":[],"execution_count":150},{"cell_type":"code","source":"class MLMDataset(Dataset):\n    def __init__(self, sentences, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.sentences = sentences\n        \n        # Cache token IDs for special tokens\n        self.CLS_TOKEN_ID = tokenizer.token_to_id(\"[CLS]\")\n        self.SEP_TOKEN_ID = tokenizer.token_to_id(\"[SEP]\")\n        self.PAD_TOKEN_ID = tokenizer.token_to_id(\"[PAD]\")\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        text = self.sentences[idx]\n        tokens = self.tokenizer.encode(text)\n\n        # Build input_ids with proper truncation\n        input_ids = [self.CLS_TOKEN_ID] + tokens.ids[:self.max_len-2] + [self.SEP_TOKEN_ID]\n        \n        # Pad to max_len\n        padding_length = self.max_len - len(input_ids)\n        input_ids += [self.PAD_TOKEN_ID] * padding_length\n\n        # Create attention mask (1 for real tokens, 0 for padding)\n        attention_mask = [1 if id != self.PAD_TOKEN_ID else 0 for id in input_ids]\n\n        # Apply masking\n        masked_input_ids, labels = mask_input(input_ids.copy(), self.tokenizer)\n\n        return {\n            'input_ids': torch.tensor(masked_input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'labels': torch.tensor(labels, dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:02:57.812703Z","iopub.execute_input":"2025-05-25T19:02:57.812969Z","iopub.status.idle":"2025-05-25T19:02:57.819555Z","shell.execute_reply.started":"2025-05-25T19:02:57.812951Z","shell.execute_reply":"2025-05-25T19:02:57.818904Z"}},"outputs":[],"execution_count":151},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# You can split sentences into train/val later\ndataset = MLMDataset(sentences, tokenizer, MAX_LEN)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Preview one batch\nbatch = next(iter(dataloader))\nprint(\"Input IDs:\", batch['input_ids'].shape)\nprint(\"Labels:\", batch['labels'].shape)\nbatch['input_ids'][0], batch['labels'][0], tokenizer.token_to_id(\"[MASK]\")\nlen(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:06.009103Z","iopub.execute_input":"2025-05-25T19:03:06.009373Z","iopub.status.idle":"2025-05-25T19:03:06.071353Z","shell.execute_reply.started":"2025-05-25T19:03:06.009354Z","shell.execute_reply":"2025-05-25T19:03:06.070657Z"}},"outputs":[{"name":"stdout","text":"Input IDs: torch.Size([32, 32])\nLabels: torch.Size([32, 32])\n","output_type":"stream"},{"execution_count":152,"output_type":"execute_result","data":{"text/plain":"300000"},"metadata":{}}],"execution_count":152},{"cell_type":"markdown","source":"# model making","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass TransformerEmbeddings(nn.Module):\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n        self.position_embed = nn.Embedding(max_len, embed_dim)\n        self.layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, input_ids):\n        seq_len = input_ids.size(1)\n        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n        x = self.token_embed(input_ids) + self.position_embed(positions)\n        return self.layer_norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:10.482218Z","iopub.execute_input":"2025-05-25T19:03:10.482869Z","iopub.status.idle":"2025-05-25T19:03:10.487521Z","shell.execute_reply.started":"2025-05-25T19:03:10.482846Z","shell.execute_reply":"2025-05-25T19:03:10.486933Z"}},"outputs":[],"execution_count":153},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, heads, ff_hidden_dim, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(embed_dim, heads, dropout=dropout, batch_first=True)\n        self.attn_norm = nn.LayerNorm(embed_dim)\n\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.GELU(),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n        self.ff_norm = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, attention_mask):\n        # attention_mask: (B, S) with 1 for real token, 0 for pad\n        # key_padding_mask needs True for positions to ignore\n        key_padding_mask = (attention_mask == 0)  # (B, S) - True for padding\n        \n        attn_output, _ = self.attn(\n            x, x, x,\n            key_padding_mask=key_padding_mask\n        )\n        \n        x = self.attn_norm(x + self.dropout(attn_output))\n        ff_output = self.ff(x)\n        x = self.ff_norm(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:14.932563Z","iopub.execute_input":"2025-05-25T19:03:14.933230Z","iopub.status.idle":"2025-05-25T19:03:14.939159Z","shell.execute_reply.started":"2025-05-25T19:03:14.933208Z","shell.execute_reply":"2025-05-25T19:03:14.938522Z"}},"outputs":[],"execution_count":154},{"cell_type":"code","source":"class MLMTransformer(nn.Module):\n    def __init__(self, vocab_size, max_len, embed_dim=128, heads=4, depth=4, ff_dim=512):\n        super().__init__()\n        self.embedding = TransformerEmbeddings(vocab_size, embed_dim, max_len)\n        self.encoder = nn.ModuleList([\n            TransformerBlock(embed_dim, heads, ff_dim) for _ in range(depth)\n        ])\n        self.mlm_head = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, input_ids, attention_mask):\n        x = self.embedding(input_ids)\n\n        # Pass attention_mask directly to each block\n        for block in self.encoder:\n            x = block(x, attention_mask)  # Fixed: pass attention_mask, not inverted\n\n        logits = self.mlm_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:17.710137Z","iopub.execute_input":"2025-05-25T19:03:17.710689Z","iopub.status.idle":"2025-05-25T19:03:17.715691Z","shell.execute_reply.started":"2025-05-25T19:03:17.710669Z","shell.execute_reply":"2025-05-25T19:03:17.715038Z"}},"outputs":[],"execution_count":155},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\n\n# Check device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:19.953819Z","iopub.execute_input":"2025-05-25T19:03:19.954287Z","iopub.status.idle":"2025-05-25T19:03:19.958704Z","shell.execute_reply.started":"2025-05-25T19:03:19.954266Z","shell.execute_reply":"2025-05-25T19:03:19.957919Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":156},{"cell_type":"code","source":"# Get vocab size from tokenizer\nvocab_size = tokenizer.get_vocab_size()\n\n# Instantiate model\nmodel = MLMTransformer(vocab_size=vocab_size, max_len=MAX_LEN).to(device)\n\n# Loss: ignore index -100 where labels are not masked\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\n# Optimizer\noptimizer = optim.AdamW(model.parameters(), lr=5e-4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:03:25.917602Z","iopub.execute_input":"2025-05-25T19:03:25.917877Z","iopub.status.idle":"2025-05-25T19:03:25.953592Z","shell.execute_reply.started":"2025-05-25T19:03:25.917857Z","shell.execute_reply":"2025-05-25T19:03:25.953052Z"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":"def train_model(model, dataloader, tokenizer, device, epochs=3):\n    vocab_size = tokenizer.get_vocab_size()\n    \n    # Lower learning rate to prevent instability\n    optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    \n    # Add learning rate scheduler\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(dataloader))\n    \n    model.train()\n    \n    for epoch in range(epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        for batch_idx, batch in enumerate(dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            logits = model(input_ids, attention_mask)\n            \n            # Calculate loss\n            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))\n            \n            # Check for NaN\n            if torch.isnan(loss):\n                print(f\"NaN detected at batch {batch_idx}\")\n                print(f\"Input IDs range: {input_ids.min()}-{input_ids.max()}\")\n                print(f\"Logits stats: min={logits.min()}, max={logits.max()}, std={logits.std()}\")\n                break\n            \n            # Backward pass with gradient clipping\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n            \n            if batch_idx % 100 == 0:\n                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n        \n        avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n\n# 6. Initialize model with proper weight initialization\ndef init_weights(m):\n    if isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            torch.nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Embedding):\n        torch.nn.init.normal_(m.weight, mean=0, std=0.02)\n\n# Usage example:\nmodel = MLMTransformer(vocab_size=vocab_size, max_len=MAX_LEN)\nmodel.apply(init_weights)  # Apply proper weight initialization\nmodel = model.to(device)\ntrain_model(model, dataloader, tokenizer, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T19:21:02.643861Z","iopub.execute_input":"2025-05-25T19:21:02.644156Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Batch 0, Loss: 9.2313\nEpoch 1, Batch 100, Loss: 9.2263\nEpoch 1, Batch 200, Loss: 9.2306\nEpoch 1, Batch 300, Loss: 9.1782\nEpoch 1, Batch 400, Loss: 9.2021\nEpoch 1, Batch 500, Loss: 9.1682\nEpoch 1, Batch 600, Loss: 9.2036\nEpoch 1, Batch 700, Loss: 9.1574\nEpoch 1, Batch 800, Loss: 9.1645\nEpoch 1, Batch 900, Loss: 9.1567\nEpoch 1, Batch 1000, Loss: 9.1302\nEpoch 1, Batch 1100, Loss: 9.0971\nEpoch 1, Batch 1200, Loss: 9.0894\nEpoch 1, Batch 1300, Loss: 9.1136\nEpoch 1, Batch 1400, Loss: 9.0638\nEpoch 1, Batch 1500, Loss: 9.0776\nEpoch 1, Batch 1600, Loss: 9.0796\nEpoch 1, Batch 1700, Loss: 9.0140\nEpoch 1, Batch 1800, Loss: 9.0639\nEpoch 1, Batch 1900, Loss: 8.9859\nEpoch 1, Batch 2000, Loss: 9.0543\nEpoch 1, Batch 2100, Loss: 8.9986\nEpoch 1, Batch 2200, Loss: 8.9959\nEpoch 1, Batch 2300, Loss: 9.0315\nEpoch 1, Batch 2400, Loss: 9.0117\nEpoch 1, Batch 2500, Loss: 9.0133\nEpoch 1, Batch 2600, Loss: 8.9877\nEpoch 1, Batch 2700, Loss: 8.9709\nEpoch 1, Batch 2800, Loss: 8.9862\nEpoch 1, Batch 2900, Loss: 8.9640\nEpoch 1, Batch 3000, Loss: 8.9650\nEpoch 1, Batch 3100, Loss: 8.9901\nEpoch 1, Batch 3200, Loss: 8.9339\nEpoch 1, Batch 3300, Loss: 8.9264\nEpoch 1, Batch 3400, Loss: 8.9680\nEpoch 1, Batch 3500, Loss: 8.8647\nEpoch 1, Batch 3600, Loss: 8.9094\nEpoch 1, Batch 3700, Loss: 8.8909\nEpoch 1, Batch 3800, Loss: 8.8728\nEpoch 1, Batch 3900, Loss: 8.8869\nEpoch 1, Batch 4000, Loss: 8.9259\nEpoch 1, Batch 4100, Loss: 8.8638\nEpoch 1, Batch 4200, Loss: 8.8528\nEpoch 1, Batch 4300, Loss: 8.8255\nEpoch 1, Batch 4400, Loss: 8.8686\nEpoch 1, Batch 4500, Loss: 8.9619\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"mlm_transformer.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:50:21.725621Z","iopub.execute_input":"2025-05-25T18:50:21.726144Z","iopub.status.idle":"2025-05-25T18:50:21.769817Z","shell.execute_reply.started":"2025-05-25T18:50:21.726121Z","shell.execute_reply":"2025-05-25T18:50:21.769040Z"}},"outputs":[],"execution_count":135},{"cell_type":"code","source":"tokenizer.save(\"mlm_tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:50:23.735634Z","iopub.execute_input":"2025-05-25T18:50:23.736323Z","iopub.status.idle":"2025-05-25T18:50:23.741411Z","shell.execute_reply.started":"2025-05-25T18:50:23.736298Z","shell.execute_reply":"2025-05-25T18:50:23.740734Z"}},"outputs":[],"execution_count":136},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tokenizers import Tokenizer\nimport numpy as np\n\nclass MLMPredictor:\n    def __init__(self, model_path, tokenizer_path, device='cuda'):\n        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n        \n        # Load tokenizer\n        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n        self.vocab_size = self.tokenizer.get_vocab_size()\n        \n        # Get special token IDs\n        self.MASK_TOKEN_ID = self.tokenizer.token_to_id(\"[MASK]\")\n        self.PAD_TOKEN_ID = self.tokenizer.token_to_id(\"[PAD]\")\n        self.CLS_TOKEN_ID = self.tokenizer.token_to_id(\"[CLS]\")\n        self.SEP_TOKEN_ID = self.tokenizer.token_to_id(\"[SEP]\")\n        print(self.MASK_TOKEN_ID)\n        # Load model\n        self.model = MLMTransformer(\n            vocab_size=self.vocab_size, \n            max_len=32,  # Make sure this matches your training config\n            embed_dim=128,\n            heads=4,\n            depth=4,\n            ff_dim=512\n        )\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.to(self.device)\n        self.model.eval()\n        \n        print(f\"Model loaded on {self.device}\")\n\n    def encode_with_mask(self, text):\n        \"\"\"\n        Tokenize text and manually replace [MASK] with correct token ID.\n        \"\"\"\n        tokens = []\n        for word in text.lower().split():\n            if word == '[mask]':\n                tokens.append(self.MASK_TOKEN_ID)\n            else:\n                encoded = self.tokenizer.encode(word)\n                tokens.extend(encoded.ids)\n        \n        return [self.CLS_TOKEN_ID] + tokens[:62] + [self.SEP_TOKEN_ID]\n\n    \n    def predict_masked_tokens(self, text, top_k=5):\n        \"\"\"\n        Predict masked tokens in the input text.\n        Text should contain [MASK] tokens where predictions are needed.\n        \"\"\"\n        # Tokenize input\n        tokens = self.tokenizer.encode(text.lower())\n        input_ids = self.encode_with_mask(text)\n        \n        # Pad to max length\n        padding_length = 32 - len(input_ids)\n        input_ids += [self.PAD_TOKEN_ID] * padding_length\n        \n        # Create attention mask\n        attention_mask = [1 if id != self.PAD_TOKEN_ID else 0 for id in input_ids]\n        \n        # Convert to tensors\n        input_ids = torch.tensor([input_ids], dtype=torch.long).to(self.device)\n        attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(self.device)\n        \n        # Get predictions\n        with torch.no_grad():\n            logits = self.model(input_ids, attention_mask)\n        \n        # Find mask positions\n        mask_positions = (input_ids[0] == self.MASK_TOKEN_ID).nonzero(as_tuple=True)[0]\n        \n        predictions = []\n        for pos in mask_positions:\n            # Get logits for this position\n            token_logits = logits[0, pos, :]\n            \n            # Get top-k predictions\n            top_k_logits, top_k_indices = torch.topk(token_logits, top_k)\n            top_k_probs = F.softmax(top_k_logits, dim=-1)\n            \n            # Convert to tokens\n            predicted_tokens = []\n            for i, (idx, prob) in enumerate(zip(top_k_indices, top_k_probs)):\n                token = self.tokenizer.decode([idx.item()])\n                predicted_tokens.append({\n                    'token': token,\n                    'probability': prob.item(),\n                    'rank': i + 1\n                })\n            \n            predictions.append({\n                'position': pos.item(),\n                'predictions': predicted_tokens\n            })\n        \n        return predictions\n    \n    def fill_mask(self, text, use_top_prediction=True):\n        \"\"\"\n        Fill [MASK] tokens in text with predictions.\n        If use_top_prediction=True, uses the highest probability token.\n        Otherwise returns multiple options.\n        \"\"\"\n        predictions = self.predict_masked_tokens(text)\n        \n        if use_top_prediction:\n            # Replace each [MASK] with top prediction\n            result_text = text\n            for pred in predictions:\n                top_token = pred['predictions'][0]['token']\n                result_text = result_text.replace('[MASK]', top_token, 1)\n            return result_text\n        else:\n            return predictions\n    \n    def interactive_prediction(self):\n        \"\"\"\n        Interactive mode for testing predictions\n        \"\"\"\n        print(\"MLM Transformer Interactive Prediction\")\n        print(\"Enter text with [MASK] tokens, or 'quit' to exit\")\n        print(\"Example: 'The cat sat on the [MASK]'\")\n        print(\"-\" * 50)\n        \n        while True:\n            text = input(\"\\nEnter masked text: \").strip()\n            \n            if text.lower() in ['quit', 'exit', 'q']:\n                break\n                \n            if '[MASK]' not in text:\n                print(\"Please include at least one [MASK] token in your text.\")\n                continue\n            \n            try:\n                # Show filled text\n                filled_text = self.fill_mask(text, use_top_prediction=True)\n                print(f\"\\nFilled text: {filled_text}\")\n                \n                # Show detailed predictions\n                predictions = self.predict_masked_tokens(text, top_k=3)\n                \n                print(f\"\\nDetailed predictions:\")\n                for i, pred in enumerate(predictions):\n                    print(f\"  [MASK] #{i+1}:\")\n                    for p in pred['predictions']:\n                        print(f\"    {p['rank']}. '{p['token']}' (prob: {p['probability']:.4f})\")\n                        \n            except Exception as e:\n                print(f\"Error during prediction: {e}\")\n\n# Example usage functions\ndef test_predictions(predictor):\n    \"\"\"\n    Test the model with some example sentences\n    \"\"\"\n    test_sentences = [\n        \"The cat sat on the [MASK]\",\n        \"I love to eat [MASK] for breakfast\",\n        \"The [MASK] is shining brightly today\",\n        \"She went to the [MASK] to buy groceries\",\n        \"The dog was [MASK] in the park\",\n        \"Python is a programming [MASK]\",\n        \"The book was very [MASK] to read\"\n    ]\n    \n    print(\"Testing MLM Predictions\")\n    print(\"=\" * 50)\n    \n    for sentence in test_sentences:\n        print(f\"\\nOriginal: {sentence}\")\n        \n        # Get filled sentence\n        filled = predictor.fill_mask(sentence)\n        print(f\"Filled:   {filled}\")\n        \n        # Get top 3 predictions for each mask\n        predictions = predictor.predict_masked_tokens(sentence, top_k=3)\n        for pred in predictions:\n            print(\"  Top predictions:\")\n            for p in pred['predictions']:\n                print(f\"    {p['rank']}. '{p['token']}' ({p['probability']:.3f})\")\n\ndef evaluate_model(predictor, test_sentences_with_answers):\n    \"\"\"\n    Evaluate model performance on known answers\n    \"\"\"\n    correct_predictions = 0\n    total_predictions = 0\n    \n    for original_sentence, masked_sentence in test_sentences_with_answers:\n        # Get prediction\n        filled = predictor.fill_mask(masked_sentence)\n        \n        # Simple accuracy check (this is basic - you might want more sophisticated evaluation)\n        if filled.lower() == original_sentence.lower():\n            correct_predictions += 1\n        total_predictions += 1\n        \n        print(f\"Original: {original_sentence}\")\n        print(f\"Masked:   {masked_sentence}\")\n        print(f\"Predicted: {filled}\")\n        print(f\"Correct: {'✓' if filled.lower() == original_sentence.lower() else '✗'}\")\n        print(\"-\" * 30)\n    \n    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n    print(f\"\\nOverall Accuracy: {accuracy:.2%} ({correct_predictions}/{total_predictions})\")\n\n# Usage example:\n\n# Load your trained model\npredictor = MLMPredictor(\n    model_path=\"mlm_transformer.pt\",\n    tokenizer_path=\"mlm_tokenizer.json\"\n)\n\n# Test with examples\ntest_predictions(predictor)\n\n# Start interactive mode\npredictor.interactive_prediction()\n\n# Or make single predictions\nresult = predictor.fill_mask(\"The weather is [MASK] today\")\nprint(result)\n\n# Get detailed predictions\npredictions = predictor.predict_masked_tokens(\"I like to [MASK] books\", top_k=5)\nfor pred in predictions:\n    print(pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T18:50:25.751477Z","iopub.execute_input":"2025-05-25T18:50:25.752139Z","iopub.status.idle":"2025-05-25T18:51:00.089034Z","shell.execute_reply.started":"2025-05-25T18:50:25.752119Z","shell.execute_reply":"2025-05-25T18:51:00.088050Z"}},"outputs":[{"name":"stdout","text":"4\nModel loaded on cuda\nTesting MLM Predictions\n==================================================\n\nOriginal: The cat sat on the [MASK]\nFilled:   The cat sat on the .\n  Top predictions:\n    1. '.' (0.617)\n    2. ',' (0.200)\n    3. 'the' (0.183)\n\nOriginal: I love to eat [MASK] for breakfast\nFilled:   I love to eat . for breakfast\n  Top predictions:\n    1. '.' (0.785)\n    2. 'the' (0.117)\n    3. 'a' (0.099)\n\nOriginal: The [MASK] is shining brightly today\nFilled:   The . is shining brightly today\n  Top predictions:\n    1. '.' (0.788)\n    2. 'the' (0.113)\n    3. 'a' (0.100)\n\nOriginal: She went to the [MASK] to buy groceries\nFilled:   She went to the . to buy groceries\n  Top predictions:\n    1. '.' (0.688)\n    2. 'the' (0.167)\n    3. 'a' (0.146)\n\nOriginal: The dog was [MASK] in the park\nFilled:   The dog was . in the park\n  Top predictions:\n    1. '.' (0.678)\n    2. 'the' (0.175)\n    3. ',' (0.147)\n\nOriginal: Python is a programming [MASK]\nFilled:   Python is a programming .\n  Top predictions:\n    1. '.' (0.830)\n    2. 'the' (0.093)\n    3. 'a' (0.077)\n\nOriginal: The book was very [MASK] to read\nFilled:   The book was very . to read\n  Top predictions:\n    1. '.' (0.836)\n    2. 'the' (0.082)\n    3. '' (0.081)\nMLM Transformer Interactive Prediction\nEnter text with [MASK] tokens, or 'quit' to exit\nExample: 'The cat sat on the [MASK]'\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter masked text:  The cat sat on the [MASK]\n"},{"name":"stdout","text":"\nFilled text: The cat sat on the .\n\nDetailed predictions:\n  [MASK] #1:\n    1. '.' (prob: 0.6168)\n    2. ',' (prob: 0.2000)\n    3. 'the' (prob: 0.1832)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3208415971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;31m# Start interactive mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteractive_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;31m# Or make single predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3208415971.py\u001b[0m in \u001b[0;36minteractive_prediction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEnter masked text: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":137}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9248118,"sourceType":"datasetVersion","datasetId":5594660}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass DecoderEmbeddings(nn.Module):\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n        self.pos_embed = nn.Embedding(max_len, embed_dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, input_ids):\n        seq_len = input_ids.size(1)\n        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)  # [1, seq_len]\n        token_embeddings = self.token_embed(input_ids)       # [batch, seq_len, dim]\n        pos_embeddings = self.pos_embed(positions)           # [1, seq_len, dim]\n        return self.dropout(token_embeddings + pos_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:04:25.717860Z","iopub.execute_input":"2025-06-01T07:04:25.718134Z","iopub.status.idle":"2025-06-01T07:04:25.723525Z","shell.execute_reply.started":"2025-06-01T07:04:25.718113Z","shell.execute_reply":"2025-06-01T07:04:25.722859Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def generate_causal_mask(seq_len, device):\n    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))  # lower triangular\n    return mask == 0  # False = allow attend, True = mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:04:34.318925Z","iopub.execute_input":"2025-06-01T07:04:34.319557Z","iopub.status.idle":"2025-06-01T07:04:34.323092Z","shell.execute_reply.started":"2025-06-01T07:04:34.319534Z","shell.execute_reply":"2025-06-01T07:04:34.322525Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n\n        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x, attn_mask=None):\n        batch_size, seq_len, embed_dim = x.size()\n\n        # Get Q, K, V\n        qkv = self.qkv_proj(x)  # [B, T, 3 * D]\n        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, T, D]\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: [B, H, T, D]\n\n        # Attention scores\n        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, T, T]\n\n        if attn_mask is not None:\n            scores = scores.masked_fill(attn_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n        attn_weights = torch.softmax(scores, dim=-1)  # [B, H, T, T]\n        attn_output = attn_weights @ v  # [B, H, T, D]\n\n        # Merge heads\n        attn_output = attn_output.transpose(1, 2).contiguous()  # [B, T, H, D]\n        attn_output = attn_output.view(batch_size, seq_len, embed_dim)\n\n        return self.out_proj(attn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:04:41.526257Z","iopub.execute_input":"2025-06-01T07:04:41.526725Z","iopub.status.idle":"2025-06-01T07:04:41.532784Z","shell.execute_reply.started":"2025-06-01T07:04:41.526701Z","shell.execute_reply":"2025-06-01T07:04:41.532150Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, embed_dim, ff_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_dim, ff_dim),\n            nn.GELU(),\n            nn.Linear(ff_dim, embed_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:04:43.639059Z","iopub.execute_input":"2025-06-01T07:04:43.639633Z","iopub.status.idle":"2025-06-01T07:04:43.643985Z","shell.execute_reply.started":"2025-06-01T07:04:43.639608Z","shell.execute_reply":"2025-06-01T07:04:43.643435Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_dim):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.ff = FeedForward(embed_dim, ff_dim)\n\n    def forward(self, x, attn_mask):\n        # Self-attention with residual\n        attn_out = self.attn(self.ln1(x), attn_mask)\n        x = x + attn_out\n\n        # Feedforward with residual\n        ff_out = self.ff(self.ln2(x))\n        x = x + ff_out\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:04:47.774368Z","iopub.execute_input":"2025-06-01T07:04:47.774916Z","iopub.status.idle":"2025-06-01T07:04:47.779438Z","shell.execute_reply.started":"2025-06-01T07:04:47.774895Z","shell.execute_reply":"2025-06-01T07:04:47.778670Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class DecoderOnlyTransformer(nn.Module):\n    def __init__(self, vocab_size, max_len, embed_dim, num_heads, depth, ff_dim):\n        super().__init__()\n        self.embedding = DecoderEmbeddings(vocab_size, embed_dim, max_len)\n\n        self.blocks = nn.ModuleList([\n            DecoderBlock(embed_dim, num_heads, ff_dim)\n            for _ in range(depth)\n        ])\n\n        self.ln_final = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, vocab_size)  # Language modeling head\n\n    def forward(self, input_ids):\n        \"\"\"\n        input_ids: [B, T]\n        \"\"\"\n        B, T = input_ids.size()\n        x = self.embedding(input_ids)  # [B, T, D]\n\n        # Generate causal mask: True where mask is applied\n        mask = generate_causal_mask(T, input_ids.device)\n\n        for block in self.blocks:\n            x = block(x, attn_mask=mask)\n\n        x = self.ln_final(x)  # [B, T, D]\n        logits = self.head(x)  # [B, T, vocab_size]\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:04:50.499702Z","iopub.execute_input":"2025-06-01T07:04:50.499986Z","iopub.status.idle":"2025-06-01T07:04:50.505953Z","shell.execute_reply.started":"2025-06-01T07:04:50.499963Z","shell.execute_reply":"2025-06-01T07:04:50.505320Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom tokenizers import ByteLevelBPETokenizer\nfrom torch.utils.data import Dataset\n\n# Parameters\ncsv_path = \"/kaggle/input/refined-bookcorpus-dataset/BookCorpus3.csv\"\nmax_paragraphs = 400_000    # Adjust based on time/memory\nmin_char_len = 200\nseq_len = 128               # Sequence length for training\n\n# 1. Load dataset\ndf = pd.read_csv(csv_path)\ndf = df.dropna()\nparagraphs = df.iloc[:max_paragraphs, 0].tolist()\n\n# 2. Filter paragraphs\nfiltered_paragraphs = [p.strip() for p in paragraphs if len(p.strip()) >= min_char_len]\n\nprint(f\"Loaded {len(filtered_paragraphs)} paragraphs after filtering.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:06:49.180879Z","iopub.execute_input":"2025-06-01T07:06:49.181364Z","iopub.status.idle":"2025-06-01T07:07:40.329752Z","shell.execute_reply.started":"2025-06-01T07:06:49.181339Z","shell.execute_reply":"2025-06-01T07:07:40.329132Z"}},"outputs":[{"name":"stdout","text":"Loaded 395901 paragraphs after filtering.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from pathlib import Path\nimport os\n# Save texts to a temporary file for tokenizer training\nwith open(\"paragraphs.txt\", \"w\", encoding=\"utf-8\") as f:\n    for p in filtered_paragraphs:\n        f.write(p + \"\\n\")\n\n# Train a ByteLevel BPE tokenizer\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files=\"paragraphs.txt\", vocab_size=30_000, min_frequency=2, special_tokens=[\n    \"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"\n])\nos.makedirs(\"tokenizer\", exist_ok=True)\n\n# Save tokenizer\ntokenizer.save_model(\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:08:20.036867Z","iopub.execute_input":"2025-06-01T07:08:20.037376Z","iopub.status.idle":"2025-06-01T07:08:48.485665Z","shell.execute_reply.started":"2025-06-01T07:08:20.037352Z","shell.execute_reply":"2025-06-01T07:08:48.484911Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['tokenizer/vocab.json', 'tokenizer/merges.txt']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from tokenizers import Tokenizer\nimport torch\n\n# Load the tokenizer from vocab + merges\ntokenizer = ByteLevelBPETokenizer(\n    \"tokenizer/vocab.json\",\n    \"tokenizer/merges.txt\"\n)\n\n\n# Tokenize entire text into one flat list of token IDs\nall_ids = []\n\nfor paragraph in filtered_paragraphs:\n    ids = tokenizer.encode(paragraph).ids\n    all_ids.extend(ids)\n\nprint(\"Total tokens:\", len(all_ids))\n\n# Split into chunks of seq_len\nsequences = []\n\nfor i in range(0, len(all_ids) - seq_len, seq_len):\n    input_ids = all_ids[i:i+seq_len]\n    sequences.append(torch.tensor(input_ids, dtype=torch.long))\n\nprint(\"Total sequences:\", len(sequences))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:08:48.486686Z","iopub.execute_input":"2025-06-01T07:08:48.486860Z","iopub.status.idle":"2025-06-01T07:10:38.620635Z","shell.execute_reply.started":"2025-06-01T07:08:48.486847Z","shell.execute_reply":"2025-06-01T07:10:38.619819Z"}},"outputs":[{"name":"stdout","text":"Total tokens: 36422547\nTotal sequences: 284551\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class CausalLanguageModelingDataset(Dataset):\n    def __init__(self, sequences):\n        self.sequences = sequences\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        x = self.sequences[idx][:-1]  # input: all except last\n        y = self.sequences[idx][1:]   # target: all except first\n        return {'input_ids': x, 'labels': y}\n\n# Create Dataset\ndataset = CausalLanguageModelingDataset(sequences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:10:38.621496Z","iopub.execute_input":"2025-06-01T07:10:38.621769Z","iopub.status.idle":"2025-06-01T07:10:38.626645Z","shell.execute_reply.started":"2025-06-01T07:10:38.621743Z","shell.execute_reply":"2025-06-01T07:10:38.626023Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n# Hyperparameters\nbatch_size = 32\nlearning_rate = 3e-4\nnum_epochs = 6\n\n# Dataloader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = DecoderOnlyTransformer(\n    vocab_size=35000,  # or tokenizer.get_vocab_size()\n    max_len=seq_len,\n    embed_dim=512,\n    num_heads=8,\n    depth=6,\n    ff_dim=2048\n).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:10:38.628002Z","iopub.execute_input":"2025-06-01T07:10:38.628212Z","iopub.status.idle":"2025-06-01T07:10:42.085651Z","shell.execute_reply.started":"2025-06-01T07:10:38.628189Z","shell.execute_reply":"2025-06-01T07:10:42.085077Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n\n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        logits = model(input_ids)\n        loss = F.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            labels.view(-1)\n        )\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Update progress bar with current batch loss\n        progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"✅ Epoch {epoch + 1}/{num_epochs} | Avg Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T07:10:42.086364Z","iopub.execute_input":"2025-06-01T07:10:42.086723Z","iopub.status.idle":"2025-06-01T09:44:11.792662Z","shell.execute_reply.started":"2025-06-01T07:10:42.086703Z","shell.execute_reply":"2025-06-01T09:44:11.791933Z"}},"outputs":[{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 1/6 | Avg Loss: 5.0705\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 2/6 | Avg Loss: 4.4116\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 3/6 | Avg Loss: 4.1843\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 4/6 | Avg Loss: 4.0414\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 5/6 | Avg Loss: 3.9364\n","output_type":"stream"},{"name":"stderr","text":"                                                                         ","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 6/6 | Avg Loss: 3.8534\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def generate(model, tokenizer, prompt, max_length=50, temperature=1, top_k=50):\n    model.eval()\n    device = next(model.parameters()).device\n\n    # Tokenize properly\n    encoding = tokenizer.encode(prompt)\n    input_ids = encoding.ids\n    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n    generated = input_ids.clone()\n\n    for _ in range(max_length):\n        logits = model(generated)  # [1, T, vocab_size]\n        next_token_logits = logits[:, -1, :] / temperature  # [1, vocab_size]\n\n        # Top-k filtering\n        if top_k is not None:\n            values, indices = torch.topk(next_token_logits, top_k)\n            mask = torch.full_like(next_token_logits, float('-inf'))\n            mask.scatter_(1, indices, values)\n            next_token_logits = mask\n\n        probs = torch.softmax(next_token_logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)  # [1, 1]\n\n        generated = torch.cat((generated, next_token), dim=1)\n\n        # Optional: Stop on EOS token if available\n        if hasattr(tokenizer, 'token_to_id') and tokenizer.token_to_id('[EOS]') is not None:\n            if next_token.item() == tokenizer.token_to_id('[EOS]'):\n                break\n\n    # Decode back to text\n    output_ids = generated[0].tolist()\n    return tokenizer.decode(output_ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:47:08.793907Z","iopub.execute_input":"2025-06-01T09:47:08.794459Z","iopub.status.idle":"2025-06-01T09:47:08.800629Z","shell.execute_reply.started":"2025-06-01T09:47:08.794436Z","shell.execute_reply":"2025-06-01T09:47:08.799935Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"generate(model, tokenizer, \"I went to eat\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\nimport os\n\n# Save tokenizer\nos.makedirs(\"hf_model\", exist_ok=True)\nfrom transformers import GPT2TokenizerFast\n\ntokenizer = GPT2TokenizerFast(\n    vocab_file=\"tokenizer/vocab.json\",\n    merges_file=\"tokenizer/merges.txt\"\n)\ntokenizer.save_pretrained(\"hf_model\")\n\n# Save model\ntorch.save(model.state_dict(), \"hf_model/pytorch_model.bin\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:49:49.514096Z","iopub.execute_input":"2025-06-01T09:49:49.514573Z","iopub.status.idle":"2025-06-01T09:49:51.391979Z","shell.execute_reply.started":"2025-06-01T09:49:49.514552Z","shell.execute_reply":"2025-06-01T09:49:51.391419Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from transformers import PreTrainedModel, PretrainedConfig\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\nclass DecoderOnlyTransformerConfig(PretrainedConfig):\n    model_type = \"decoder-only-transformer\"\n\n    def __init__(self, vocab_size=35000, max_len=256, embed_dim=512,\n                 num_heads=8, depth=6, ff_dim=2048, **kwargs):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.depth = depth\n        self.ff_dim = ff_dim\n\nclass HFDecoderOnlyTransformer(PreTrainedModel):\n    config_class = DecoderOnlyTransformerConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = DecoderOnlyTransformer(\n            vocab_size=config.vocab_size,\n            max_len=config.max_len,\n            embed_dim=config.embed_dim,\n            num_heads=config.num_heads,\n            depth=config.depth,\n            ff_dim=config.ff_dim,\n        )\n\n    def forward(self, input_ids, labels=None):\n        logits = self.model(input_ids)\n        loss = None\n        if labels is not None:\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n        return {\"loss\": loss, \"logits\": logits}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:50:46.215233Z","iopub.execute_input":"2025-06-01T09:50:46.216022Z","iopub.status.idle":"2025-06-01T09:51:03.158046Z","shell.execute_reply.started":"2025-06-01T09:50:46.215993Z","shell.execute_reply":"2025-06-01T09:51:03.157516Z"}},"outputs":[{"name":"stderr","text":"2025-06-01 09:50:51.281636: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748771451.470752      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748771451.534404      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import json\n# Create config.json\nconfig = {\n    \"model_type\": \"decoder-only-transformer\",\n    \"vocab_size\": 35000,\n    \"max_len\": 256,\n    \"embed_dim\": 512,\n    \"num_heads\": 8,\n    \"depth\": 6,\n    \"ff_dim\": 2048\n}\nwith open(os.path.join(\"hf_model\", \"config.json\"), \"w\") as f:\n    json.dump(config, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:53:35.666118Z","iopub.execute_input":"2025-06-01T09:53:35.666775Z","iopub.status.idle":"2025-06-01T09:53:35.671013Z","shell.execute_reply.started":"2025-06-01T09:53:35.666754Z","shell.execute_reply":"2025-06-01T09:53:35.670462Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('')\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T09:58:36.774294Z","iopub.execute_input":"2025-06-01T09:58:36.775190Z","iopub.status.idle":"2025-06-01T09:58:37.525497Z","shell.execute_reply.started":"2025-06-01T09:58:36.775130Z","shell.execute_reply":"2025-06-01T09:58:37.524501Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"%cd /kaggle/working/hf_model\n!git init","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:04:01.659251Z","iopub.execute_input":"2025-06-01T10:04:01.659688Z","iopub.status.idle":"2025-06-01T10:04:02.096137Z","shell.execute_reply.started":"2025-06-01T10:04:01.659655Z","shell.execute_reply":"2025-06-01T10:04:02.095412Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/hf_model\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n\u001b[33mhint: \u001b[m\n\u001b[33mhint: \tgit branch -m <name>\u001b[m\nInitialized empty Git repository in /kaggle/working/hf_model/.git/\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!git config user.email \"designsdilip@gmail.com\"\n!git config user.name \"coder-dilip\"\n\n!git remote add origin https://<hf token>@huggingface.co/dilip025/mini-gpt1\n!git branch -M main\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:04:31.302498Z","iopub.execute_input":"2025-06-01T10:04:31.303094Z","iopub.status.idle":"2025-06-01T10:04:32.924564Z","shell.execute_reply.started":"2025-06-01T10:04:31.303062Z","shell.execute_reply":"2025-06-01T10:04:32.923813Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!git add .\n!git commit -m \"Initial commit\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:04:45.468851Z","iopub.execute_input":"2025-06-01T10:04:45.469214Z","iopub.status.idle":"2025-06-01T10:04:57.396026Z","shell.execute_reply.started":"2025-06-01T10:04:45.469153Z","shell.execute_reply":"2025-06-01T10:04:57.395212Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"[main (root-commit) 11330cc] Initial commit\n 8 files changed, 178804 insertions(+)\n create mode 100644 added_tokens.json\n create mode 100644 config.json\n create mode 100644 merges.txt\n create mode 100644 pytorch_model.bin\n create mode 100644 special_tokens_map.json\n create mode 100644 tokenizer.json\n create mode 100644 tokenizer_config.json\n create mode 100644 vocab.json\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\ntoken = \"<hf_token>\"\nrepo_id = \"dilip025/mini-gpt1\"\n\napi = HfApi()\napi.create_repo(repo_id=repo_id, token=token, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:08:50.485671Z","iopub.execute_input":"2025-06-01T10:08:50.485963Z","iopub.status.idle":"2025-06-01T10:08:50.621784Z","shell.execute_reply.started":"2025-06-01T10:08:50.485941Z","shell.execute_reply":"2025-06-01T10:08:50.621227Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/dilip025/mini-gpt1', endpoint='https://huggingface.co', repo_type='model', repo_id='dilip025/mini-gpt1')"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"!rm -rf /kaggle/working/hf_model/.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:10:33.708964Z","iopub.execute_input":"2025-06-01T10:10:33.709300Z","iopub.status.idle":"2025-06-01T10:10:34.159091Z","shell.execute_reply.started":"2025-06-01T10:10:33.709276Z","shell.execute_reply":"2025-06-01T10:10:34.158394Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"from huggingface_hub import upload_file\nimport os\n\nfolder_path = \"/kaggle/working/hf_model\"\n\nfor root, dirs, files in os.walk(folder_path):\n    for file in files:\n        local_path = os.path.join(root, file)\n        remote_path = os.path.relpath(local_path, folder_path)\n        upload_file(\n            path_or_fileobj=local_path,\n            path_in_repo=remote_path,\n            repo_id=repo_id,\n            token=token\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:17:14.220475Z","iopub.execute_input":"2025-06-01T10:17:14.221343Z","iopub.status.idle":"2025-06-01T10:17:19.464282Z","shell.execute_reply.started":"2025-06-01T10:17:14.221309Z","shell.execute_reply":"2025-06-01T10:17:19.463711Z"}},"outputs":[{"name":"stderr","text":"No files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\nNo files have been modified since last commit. Skipping to prevent empty commit.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import json\nimport os\n\n# Define the config dictionary\nconfig = {\n    \"model_type\": \"decoder-only-transformer\",\n    \"vocab_size\": 35000,\n    \"max_len\": 256,\n    \"embed_dim\": 512,\n    \"num_heads\": 8,\n    \"depth\": 6,\n    \"ff_dim\": 2048\n}\n\n# Create hf_model directory if it doesn't exist\nos.makedirs(\"hf_model\", exist_ok=True)\n\n# Write config.json\nwith open(\"hf_model/config.json\", \"w\") as f:\n    json.dump(config, f, indent=4)\n\n# Create tokenizer_config.json (needed for Hugging Face)\ntokenizer_config = {\n    \"add_prefix_space\": True,\n    \"model_max_length\": 256,\n    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n    \"unk_token\": \"<unk>\",\n    \"bos_token\": \"<s>\",\n    \"eos_token\": \"</s>\"\n}\n\nwith open(\"hf_model/tokenizer_config.json\", \"w\") as f:\n    json.dump(tokenizer_config, f, indent=4)\n\n# Create README.md\nreadme = \"\"\"# Mini GPT1 Clone\n\nThis is a decoder-only transformer model (GPT1-style) trained from scratch using PyTorch.\n\n## Model Details\n\n- **Architecture**: Decoder-only Transformer\n- **Layers**: 6\n- **Embedding Size**: 512\n- **Heads**: 8\n- **Feedforward Dim**: 2048\n- **Sequence Length**: 256\n- **Vocab Size**: 35,000\n\n## Tokenizer\n\nTrained using `ByteLevelBPETokenizer` from the `tokenizers` library.\n\n## Inference Example\n\n```python\nfrom transformers import PreTrainedTokenizerFast, AutoModelForCausalLM\nimport torch\n\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer/tokenizer.json\")\nmodel = AutoModelForCausalLM.from_pretrained(\"dilip025/mini-gpt1\")\n\nprompt = \"Once upon a time,\"\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\noutputs = model.generate(input_ids, max_length=50)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\nLicense\nMIT\n\"\"\"\n\nwith open(\"hf_model/README.md\", \"w\") as f:\n    f.write(readme)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T10:16:38.740824Z","iopub.execute_input":"2025-06-01T10:16:38.741142Z","iopub.status.idle":"2025-06-01T10:16:38.748681Z","shell.execute_reply.started":"2025-06-01T10:16:38.741120Z","shell.execute_reply":"2025-06-01T10:16:38.747997Z"}},"outputs":[],"execution_count":45}]}
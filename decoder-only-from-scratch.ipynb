{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9248118,"sourceType":"datasetVersion","datasetId":5594660}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:08:34.926263Z","iopub.execute_input":"2025-05-31T17:08:34.926968Z","iopub.status.idle":"2025-05-31T17:08:34.934566Z","shell.execute_reply.started":"2025-05-31T17:08:34.926944Z","shell.execute_reply":"2025-05-31T17:08:34.933840Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/refined-bookcorpus-dataset/BookCorpus3.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass DecoderEmbeddings(nn.Module):\n    def __init__(self, vocab_size, embed_dim, max_len):\n        super().__init__()\n        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n        self.pos_embed = nn.Embedding(max_len, embed_dim)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, input_ids):\n        seq_len = input_ids.size(1)\n        positions = torch.arange(0, seq_len, device=input_ids.device).unsqueeze(0)  # [1, seq_len]\n        token_embeddings = self.token_embed(input_ids)       # [batch, seq_len, dim]\n        pos_embeddings = self.pos_embed(positions)           # [1, seq_len, dim]\n        return self.dropout(token_embeddings + pos_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:08:38.110755Z","iopub.execute_input":"2025-05-31T17:08:38.111447Z","iopub.status.idle":"2025-05-31T17:08:38.116210Z","shell.execute_reply.started":"2025-05-31T17:08:38.111421Z","shell.execute_reply":"2025-05-31T17:08:38.115575Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def generate_causal_mask(seq_len, device):\n    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))  # lower triangular\n    return mask == 0  # False = allow attend, True = mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:08:40.233100Z","iopub.execute_input":"2025-05-31T17:08:40.233357Z","iopub.status.idle":"2025-05-31T17:08:40.237590Z","shell.execute_reply.started":"2025-05-31T17:08:40.233338Z","shell.execute_reply":"2025-05-31T17:08:40.236741Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n\n        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x, attn_mask=None):\n        batch_size, seq_len, embed_dim = x.size()\n\n        # Get Q, K, V\n        qkv = self.qkv_proj(x)  # [B, T, 3 * D]\n        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, T, D]\n        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: [B, H, T, D]\n\n        # Attention scores\n        scores = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [B, H, T, T]\n\n        if attn_mask is not None:\n            scores = scores.masked_fill(attn_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n        attn_weights = torch.softmax(scores, dim=-1)  # [B, H, T, T]\n        attn_output = attn_weights @ v  # [B, H, T, D]\n\n        # Merge heads\n        attn_output = attn_output.transpose(1, 2).contiguous()  # [B, T, H, D]\n        attn_output = attn_output.view(batch_size, seq_len, embed_dim)\n\n        return self.out_proj(attn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:18:00.990779Z","iopub.execute_input":"2025-05-31T17:18:00.991405Z","iopub.status.idle":"2025-05-31T17:18:00.998804Z","shell.execute_reply.started":"2025-05-31T17:18:00.991381Z","shell.execute_reply":"2025-05-31T17:18:00.997987Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, embed_dim, ff_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(embed_dim, ff_dim),\n            nn.GELU(),\n            nn.Linear(ff_dim, embed_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:18:03.378135Z","iopub.execute_input":"2025-05-31T17:18:03.378400Z","iopub.status.idle":"2025-05-31T17:18:03.382873Z","shell.execute_reply.started":"2025-05-31T17:18:03.378381Z","shell.execute_reply":"2025-05-31T17:18:03.382225Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_dim):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.ff = FeedForward(embed_dim, ff_dim)\n\n    def forward(self, x, attn_mask):\n        # Self-attention with residual\n        attn_out = self.attn(self.ln1(x), attn_mask)\n        x = x + attn_out\n\n        # Feedforward with residual\n        ff_out = self.ff(self.ln2(x))\n        x = x + ff_out\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:18:05.692165Z","iopub.execute_input":"2025-05-31T17:18:05.692411Z","iopub.status.idle":"2025-05-31T17:18:05.697835Z","shell.execute_reply.started":"2025-05-31T17:18:05.692393Z","shell.execute_reply":"2025-05-31T17:18:05.697038Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class DecoderOnlyTransformer(nn.Module):\n    def __init__(self, vocab_size, max_len, embed_dim, num_heads, depth, ff_dim):\n        super().__init__()\n        self.embedding = DecoderEmbeddings(vocab_size, embed_dim, max_len)\n\n        self.blocks = nn.ModuleList([\n            DecoderBlock(embed_dim, num_heads, ff_dim)\n            for _ in range(depth)\n        ])\n\n        self.ln_final = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, vocab_size)  # Language modeling head\n\n    def forward(self, input_ids):\n        \"\"\"\n        input_ids: [B, T]\n        \"\"\"\n        B, T = input_ids.size()\n        x = self.embedding(input_ids)  # [B, T, D]\n\n        # Generate causal mask: True where mask is applied\n        mask = generate_causal_mask(T, input_ids.device)\n\n        for block in self.blocks:\n            x = block(x, attn_mask=mask)\n\n        x = self.ln_final(x)  # [B, T, D]\n        logits = self.head(x)  # [B, T, vocab_size]\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:18:11.828889Z","iopub.execute_input":"2025-05-31T17:18:11.829162Z","iopub.status.idle":"2025-05-31T17:18:11.835340Z","shell.execute_reply.started":"2025-05-31T17:18:11.829143Z","shell.execute_reply":"2025-05-31T17:18:11.834466Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import pandas as pd\nfrom tokenizers import ByteLevelBPETokenizer\nfrom torch.utils.data import Dataset\n\n# Parameters\ncsv_path = \"/kaggle/input/refined-bookcorpus-dataset/BookCorpus3.csv\"\nmax_paragraphs = 200_000    # Adjust based on time/memory\nmin_char_len = 200\nseq_len = 128               # Sequence length for training\n\n# 1. Load dataset\ndf = pd.read_csv(csv_path)\ndf = df.dropna()\nparagraphs = df.iloc[:max_paragraphs, 0].tolist()\n\n# 2. Filter paragraphs\nfiltered_paragraphs = [p.strip() for p in paragraphs if len(p.strip()) >= min_char_len]\n\nprint(f\"Loaded {len(filtered_paragraphs)} paragraphs after filtering.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:08:48.720166Z","iopub.execute_input":"2025-05-31T17:08:48.720736Z","iopub.status.idle":"2025-05-31T17:10:11.578201Z","shell.execute_reply.started":"2025-05-31T17:08:48.720716Z","shell.execute_reply":"2025-05-31T17:10:11.577548Z"}},"outputs":[{"name":"stdout","text":"Loaded 198159 paragraphs after filtering.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from pathlib import Path\n\n# Save texts to a temporary file for tokenizer training\nwith open(\"paragraphs.txt\", \"w\", encoding=\"utf-8\") as f:\n    for p in filtered_paragraphs:\n        f.write(p + \"\\n\")\n\n# Train a ByteLevel BPE tokenizer\ntokenizer = ByteLevelBPETokenizer()\ntokenizer.train(files=\"paragraphs.txt\", vocab_size=30_000, min_frequency=2, special_tokens=[\n    \"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"\n])\nos.makedirs(\"tokenizer\", exist_ok=True)\n\n# Save tokenizer\ntokenizer.save_model(\"tokenizer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:10:11.579450Z","iopub.execute_input":"2025-05-31T17:10:11.579692Z","iopub.status.idle":"2025-05-31T17:10:27.810534Z","shell.execute_reply.started":"2025-05-31T17:10:11.579676Z","shell.execute_reply":"2025-05-31T17:10:27.810005Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['tokenizer/vocab.json', 'tokenizer/merges.txt']"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from tokenizers import Tokenizer\nimport torch\n\n# Load the tokenizer from vocab + merges\ntokenizer = ByteLevelBPETokenizer(\n    \"tokenizer/vocab.json\",\n    \"tokenizer/merges.txt\"\n)\n\n\n# Tokenize entire text into one flat list of token IDs\nall_ids = []\n\nfor paragraph in filtered_paragraphs:\n    ids = tokenizer.encode(paragraph).ids\n    all_ids.extend(ids)\n\nprint(\"Total tokens:\", len(all_ids))\n\n# Split into chunks of seq_len\nsequences = []\n\nfor i in range(0, len(all_ids) - seq_len, seq_len):\n    input_ids = all_ids[i:i+seq_len]\n    sequences.append(torch.tensor(input_ids, dtype=torch.long))\n\nprint(\"Total sequences:\", len(sequences))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:10:27.811245Z","iopub.execute_input":"2025-05-31T17:10:27.811494Z","iopub.status.idle":"2025-05-31T17:11:18.912712Z","shell.execute_reply.started":"2025-05-31T17:10:27.811476Z","shell.execute_reply":"2025-05-31T17:11:18.912087Z"}},"outputs":[{"name":"stdout","text":"Total tokens: 18868576\nTotal sequences: 147410\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class CausalLanguageModelingDataset(Dataset):\n    def __init__(self, sequences):\n        self.sequences = sequences\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        x = self.sequences[idx][:-1]  # input: all except last\n        y = self.sequences[idx][1:]   # target: all except first\n        return {'input_ids': x, 'labels': y}\n\n# Create Dataset\ndataset = CausalLanguageModelingDataset(sequences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:11:18.914350Z","iopub.execute_input":"2025-05-31T17:11:18.914744Z","iopub.status.idle":"2025-05-31T17:11:18.919323Z","shell.execute_reply.started":"2025-05-31T17:11:18.914724Z","shell.execute_reply":"2025-05-31T17:11:18.918654Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\n# Hyperparameters\nbatch_size = 32\nlearning_rate = 3e-4\nnum_epochs = 10\n\n# Dataloader\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = DecoderOnlyTransformer(\n    vocab_size=30000,  # or tokenizer.get_vocab_size()\n    max_len=seq_len,\n    embed_dim=512,\n    num_heads=8,\n    depth=6,\n    ff_dim=2048\n).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:18:25.177420Z","iopub.execute_input":"2025-05-31T17:18:25.178091Z","iopub.status.idle":"2025-05-31T17:18:25.658417Z","shell.execute_reply.started":"2025-05-31T17:18:25.178063Z","shell.execute_reply":"2025-05-31T17:18:25.657640Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from tqdm import tqdm\n\nmodel.train()\n\nfor epoch in range(num_epochs):\n    total_loss = 0\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", leave=False)\n\n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n\n        logits = model(input_ids)\n        loss = F.cross_entropy(\n            logits.view(-1, logits.size(-1)),\n            labels.view(-1)\n        )\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Update progress bar with current batch loss\n        progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(dataloader)\n    print(f\"✅ Epoch {epoch + 1}/{num_epochs} | Avg Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T17:20:59.788556Z","iopub.execute_input":"2025-05-31T17:20:59.788882Z"}},"outputs":[{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 1/10 | Avg Loss: 5.2244\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 2/10 | Avg Loss: 4.5440\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10:  20%|██        | 933/4607 [02:32<10:03,  6.08it/s, loss=4.22]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def generate(model, tokenizer, prompt, max_new_tokens=50):\n    model.eval()\n    input_ids = tokenizer.encode(prompt).ids\n    input_tensor = torch.tensor([input_ids], device=device)\n\n    for _ in range(max_new_tokens):\n        with torch.no_grad():\n            logits = model(input_tensor)\n            next_token_logits = logits[0, -1, :]\n            next_token = torch.argmax(next_token_logits).item()\n            input_tensor = torch.cat([input_tensor, torch.tensor([[next_token]], device=device)], dim=1)\n\n    return tokenizer.decode(input_tensor[0].tolist())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate(model, tokenizer, \"hello\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}